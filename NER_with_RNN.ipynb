{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "edf7e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from seqeval.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c9a3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_file(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with lzma.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                tags_part, tokens_part = line.split('\\t', 1)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            tag_seq = tags_part.split()\n",
    "            token_seq = tokens_part.split()\n",
    "            if len(tag_seq) != len(token_seq):\n",
    "                continue\n",
    "            sentences.append(token_seq)\n",
    "            labels.append(tag_seq)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a4458267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(sentences, labels, test_ratio=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    combined = list(zip(sentences, labels))\n",
    "    random.shuffle(combined)\n",
    "    split_idx = int(len(combined) * (1 - test_ratio))\n",
    "    train_data = combined[:split_idx]\n",
    "    test_data = combined[split_idx:]\n",
    "    train_sents, train_tags = zip(*train_data)\n",
    "    test_sents, test_tags = zip(*test_data)\n",
    "    return list(train_sents), list(train_tags), list(test_sents), list(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ffd58202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, word2idx, tag2idx, max_len=50):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.sentences[idx]\n",
    "        tag_seq = self.labels[idx]\n",
    "\n",
    "        x = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
    "        y = [self.tag2idx.get(t, self.tag2idx['O']) for t in tag_seq]\n",
    "\n",
    "        if len(x) < self.max_len:\n",
    "            pad_len = self.max_len - len(x)\n",
    "            x += [self.word2idx['<PAD>']] * pad_len\n",
    "            y += [self.tag2idx['<PAD>']] * pad_len\n",
    "        else:\n",
    "            x = x[:self.max_len]\n",
    "            y = y[:self.max_len]\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "40b9fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "19e2bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 850\n",
      "Test sentences: 95\n",
      "Sample sentence:\n",
      "['Saudi', 'Arabia', 'executes', 'Pakistani', 'man', '.', '</S>', 'DUBAI', '1996-08-25', '</S>', 'Saudi', 'Arabia', 'executed', 'on', 'Sunday', 'a', 'Pakistani', 'man', 'accused', 'of', 'belonging', 'to', 'an', 'armed', 'gang', 'of', 'robbers', ',', 'Saudi', 'television', 'reported', '.', '</S>', 'It', 'quoted', 'an', 'Interior', 'Ministry', 'statement', 'as', 'saying', 'Shabir', 'Ahmad', 'Muhammad', 'Jalil', 'was', 'executed', 'in', 'Mecca', '.', '</S>', 'He', 'was', 'the', '26th', 'person', 'executed', 'this', 'year', 'in', 'the', 'kingdom', '.', '</S>', 'Saudi', 'Arabia', 'beheads', 'convicted', 'drug', 'smugglers', ',', 'rapists', ',', 'murderers', 'and', 'other', 'criminals', '.', '</S>']\n",
      "Sample labels:\n",
      "['B-LOC', 'I-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "#https://git.wmi.amu.edu.pl/kubapok/en-ner-conll-2003\n",
    "train_sents, train_tags = parse_train_file('data/en-ner-conll-2003/train/train.tsv.xz')\n",
    "train_sents, train_tags, test_sents, test_tags = train_test_split(train_sents, train_tags, test_ratio=0.1)\n",
    "\n",
    "print(f\"Train sentences: {len(train_sents)}\")\n",
    "print(f\"Test sentences: {len(test_sents)}\")\n",
    "print(\"Sample sentence:\")\n",
    "print(train_sents[0])\n",
    "print(\"Sample labels:\")\n",
    "print(train_tags[0])\n",
    "\n",
    "# 6. Budujemy słowniki\n",
    "all_words = set(w for sent in train_sents for w in sent)\n",
    "all_tags = set(t for tags in train_tags for t in tags)\n",
    "all_tags.add('<PAD>')  # dodajemy pad do tagów\n",
    "\n",
    "word2idx = {w: i+2 for i, w in enumerate(sorted(all_words))}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(sorted(all_tags))}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "92051de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "train_dataset = NERDataset(train_sents, train_tags, word2idx, tag2idx, max_len=max_len)\n",
    "test_dataset = NERDataset(test_sents, test_tags, word2idx, tag2idx, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "244fff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "591b677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 1.6933\n",
      "Epoch 2/30 - Loss: 0.8315\n",
      "Epoch 3/30 - Loss: 0.7484\n",
      "Epoch 4/30 - Loss: 0.7033\n",
      "Epoch 5/30 - Loss: 0.6580\n",
      "Epoch 6/30 - Loss: 0.6035\n",
      "Epoch 7/30 - Loss: 0.5466\n",
      "Epoch 8/30 - Loss: 0.4910\n",
      "Epoch 9/30 - Loss: 0.4370\n",
      "Epoch 10/30 - Loss: 0.3825\n",
      "Epoch 11/30 - Loss: 0.3334\n",
      "Epoch 12/30 - Loss: 0.2898\n",
      "Epoch 13/30 - Loss: 0.2501\n",
      "Epoch 14/30 - Loss: 0.2179\n",
      "Epoch 15/30 - Loss: 0.1896\n",
      "Epoch 16/30 - Loss: 0.1654\n",
      "Epoch 17/30 - Loss: 0.1455\n",
      "Epoch 18/30 - Loss: 0.1278\n",
      "Epoch 19/30 - Loss: 0.1122\n",
      "Epoch 20/30 - Loss: 0.0980\n",
      "Epoch 21/30 - Loss: 0.0861\n",
      "Epoch 22/30 - Loss: 0.0760\n",
      "Epoch 23/30 - Loss: 0.0664\n",
      "Epoch 24/30 - Loss: 0.0582\n",
      "Epoch 25/30 - Loss: 0.0519\n",
      "Epoch 26/30 - Loss: 0.0455\n",
      "Epoch 27/30 - Loss: 0.0406\n",
      "Epoch 28/30 - Loss: 0.0355\n",
      "Epoch 29/30 - Loss: 0.0316\n",
      "Epoch 30/30 - Loss: 0.0280\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        y_batch = y_batch.view(-1)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "135d2f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.70      0.75      0.72       360\n",
      "        MISC       0.65      0.66      0.66       211\n",
      "         ORG       0.47      0.58      0.52       303\n",
      "         PER       0.62      0.47      0.54       206\n",
      "\n",
      "   micro avg       0.60      0.63      0.62      1080\n",
      "   macro avg       0.61      0.61      0.61      1080\n",
      "weighted avg       0.61      0.63      0.62      1080\n",
      "\n",
      "F1 score: 0.6171273221567739\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            preds = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
    "            y_true = y_batch.numpy()\n",
    "\n",
    "            for pred_seq, true_seq in zip(preds, y_true):\n",
    "                length = sum(t != tag2idx['<PAD>'] for t in true_seq)\n",
    "                pred_tags = [idx2tag[idx] for idx in pred_seq[:length]]\n",
    "                true_tags = [idx2tag[idx] for idx in true_seq[:length]]\n",
    "\n",
    "                all_preds.append(pred_tags)\n",
    "                all_labels.append(true_tags)\n",
    "\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    print(\"F1 score:\", f1_score(all_labels, all_preds))\n",
    "\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cea25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
